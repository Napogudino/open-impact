{
    "docs": [
        {
            "location": "/planet-asset/", 
            "text": "Understand Planet Items-Assets \n API\n\n\nYou can read the most updated white paper on planet products, items, assets and specifications \nhere\n. While you look at these spec sheets try to understand how you would want to use the data, the purpose and scope of the question you want to answer, the size of downloads and the overall product or derivate in mind. To think of Planet products you have to understand two terms as thought they live in a hierarchy\n\n\n\n\nPlanet Imagery Product Offerings\n\n\nItems and Assets\n\n\n\n\n\n\nItem type almost refers exclusively to a family of satellite or sensor types so PlanetScope, RapidEye, Skysat, Landsat and so on are all item types. These are model definitions based on the type of sensor you are utilizing for performing any type of analysis.\n\n\n\n\n\n\nAsset types are types of item derivatives or data types that you are actually utilizing for example analytic, analytic_sr, analytic_xml, visual and so on. These allow you to choose the type of actual data that you are able to download including the type and level of preprocesing that has been applied to it.\n\n\n\n\n\n\nFor further reference on item asset relationships you can \nvisit the docs\n\n\nNow the assumption here is that after you have created your account you have downloaded data either from \nPlanet Explorer\n or you have been curious and looked into the data API and used the wonderful \npython client\n from planet. Incase you have not and you have python on your system, invoke the power of pip and type\n\n\npip install planet\n\n\nThere is so much more to be done using planet data using some amazing API(s) including\n\n\n\n\n\n\nWhat if you wanted to download hundreds and thousands of scenes for your analysis, the \nData API\n will allow you to understand the backend.\n\n\n\n\n\n\nIf you want your images to be automatically clipped to your area of interest, you can use the \nClips API", 
            "title": "Planet Imagery"
        }, 
        {
            "location": "/planet-asset/#understand-planet-items-assets-api", 
            "text": "You can read the most updated white paper on planet products, items, assets and specifications  here . While you look at these spec sheets try to understand how you would want to use the data, the purpose and scope of the question you want to answer, the size of downloads and the overall product or derivate in mind. To think of Planet products you have to understand two terms as thought they live in a hierarchy   Planet Imagery Product Offerings", 
            "title": "Understand Planet Items-Assets &amp; API"
        }, 
        {
            "location": "/planet-asset/#items-and-assets", 
            "text": "Item type almost refers exclusively to a family of satellite or sensor types so PlanetScope, RapidEye, Skysat, Landsat and so on are all item types. These are model definitions based on the type of sensor you are utilizing for performing any type of analysis.    Asset types are types of item derivatives or data types that you are actually utilizing for example analytic, analytic_sr, analytic_xml, visual and so on. These allow you to choose the type of actual data that you are able to download including the type and level of preprocesing that has been applied to it.    For further reference on item asset relationships you can  visit the docs  Now the assumption here is that after you have created your account you have downloaded data either from  Planet Explorer  or you have been curious and looked into the data API and used the wonderful  python client  from planet. Incase you have not and you have python on your system, invoke the power of pip and type  pip install planet  There is so much more to be done using planet data using some amazing API(s) including    What if you wanted to download hundreds and thousands of scenes for your analysis, the  Data API  will allow you to understand the backend.    If you want your images to be automatically clipped to your area of interest, you can use the  Clips API", 
            "title": "Items and Assets"
        }, 
        {
            "location": "/", 
            "text": "Introduction\n\n\nAs Planet achieved \nmission one\n last year it started to map the entire planet every single day. With this came the need to store tremendous amount of data, being able to serve it using our faboulous \nPlanet Explorer\n and for you be able to dowload it after you created an account. While getting imagery became critically important , the delivery method lead to questions about platforms for analysis. Mission one further meant that you would be able to analyze time series datasets over weekly and bi weekly periods and understand questions that require dynamic analysis such as vegetation patters, climate variability and changes in hydrological cycles. It generated the single largest trove of high frequency dataset in existence with an unprecedented cadence or repeat over areas.\n\n\n\n\nPlanet by the numbers from \nWill Marshall\u2019s post\n \u00a9 Planet Labs\n\n\nWith the need to handle such global-scale and often massive data sets, Google was already building and hosting a platform called \nGoogle Earth Engine(GEE)\n designed to analyze geo-spatial data. Google Earth Engine is a cloud based analysis platform that allows for an indexable and queriable raster and vector environment while including and expanding on raster capabilities. It allows for users to perform large scale analysis on large scale datasets such as Planet's very own. You can read more about the Google Earth Engine platform \nhere\n\n\n\n\nThese set of tutorials will introduce you to methods in getting imagery onto the Google Earth Engine platform, and how do you quickly visualize and analyze the datasets with the GEE platform. With the multitude of tools and possibilities of building your own analysis tools. The first of it's kind tools will include basic methods such as 4 band operations and can include complex operations in machine learning as the platform evolves.\n\n\nFor more information and developer guide on Google Earth Engine you can visit their \ndeveloper console\n.", 
            "title": "Planet & Google Earth Engine"
        }, 
        {
            "location": "/#introduction", 
            "text": "As Planet achieved  mission one  last year it started to map the entire planet every single day. With this came the need to store tremendous amount of data, being able to serve it using our faboulous  Planet Explorer  and for you be able to dowload it after you created an account. While getting imagery became critically important , the delivery method lead to questions about platforms for analysis. Mission one further meant that you would be able to analyze time series datasets over weekly and bi weekly periods and understand questions that require dynamic analysis such as vegetation patters, climate variability and changes in hydrological cycles. It generated the single largest trove of high frequency dataset in existence with an unprecedented cadence or repeat over areas.   Planet by the numbers from  Will Marshall\u2019s post  \u00a9 Planet Labs  With the need to handle such global-scale and often massive data sets, Google was already building and hosting a platform called  Google Earth Engine(GEE)  designed to analyze geo-spatial data. Google Earth Engine is a cloud based analysis platform that allows for an indexable and queriable raster and vector environment while including and expanding on raster capabilities. It allows for users to perform large scale analysis on large scale datasets such as Planet's very own. You can read more about the Google Earth Engine platform  here   These set of tutorials will introduce you to methods in getting imagery onto the Google Earth Engine platform, and how do you quickly visualize and analyze the datasets with the GEE platform. With the multitude of tools and possibilities of building your own analysis tools. The first of it's kind tools will include basic methods such as 4 band operations and can include complex operations in machine learning as the platform evolves.  For more information and developer guide on Google Earth Engine you can visit their  developer console .", 
            "title": "Introduction"
        }, 
        {
            "location": "/projects/rpl/", 
            "text": "Registering for a Planet account\n\n\nWhat you need first to get started in simply to register for a Planet account. These account will almost immediately gain access to the Open California Dataset which is maintained regulary and is perhaps one of the largest open imagery dataset at this spatial and temporal resolution. You can find more information about the \nOpen California project here\n. These datasets and full-resolution imagery for the entire state of California are covered under a CC BY-SA 4.0 license via Planet's Open California initiative. Your free account allows you to download about 2 GB of dataset each day.\n\n\nSign up for a Planet Account\n\n\nPlanet Explorer\n is a powerful tool for exploring Planet's catalog of daily imagery and worldwide mosaics\ndirectly in your browser. It's also your gateway to creating a Planet Account,and gaining access to Planet's APIs.To sign up, visit \nplanet.com/explorer\n and click \nGet Started\n:\n\n\n\n\nGet started with Planet Explorer\nFrom there, click \nSign Up\n in the top right of your screen, and enter your email address to receive an invitation:\n\n\n\n\nSign up with Planet Explorer\n\n\nCheck your email \n follow the directions to complete the registration process.\n\n\nFind your API Key\n\n\nTo use Planet's APIs, you'll need an API key. API keys are available to all registered users with active Planet accounts.Once you're signed up, log in to\n\nplanet.com/account\n to get your API key. Find the \nAPI key\n field under your account information, as seen here:\n\n\n\n\nAccount information (not a real API key)\n\n\nRegistering for a Google Earth Engine Account\n\n\nIf you don\u2019t have a developer account \nsign up for one here\n and make sure you follow the \ninstructions\n to install the python CLI.\n\n\n\n\nThe API and the CLI gets updated frequently and as does the install process as needed so you can read the latest instructions at the page.\n\n\nGetting Help with Planet and Google Earth Engine\n\n\nBoth Planet and Google Earth Engine maintain a developer page for you to find out more information,test tutorials along with housing a few quick FAQ(s)\n\n\nYou can find \nPlanet Developer Site here\n\n\nand offcourse the \nEarth Engine Developers Page", 
            "title": "Setting up your Accounts"
        }, 
        {
            "location": "/projects/rpl/#registering-for-a-planet-account", 
            "text": "What you need first to get started in simply to register for a Planet account. These account will almost immediately gain access to the Open California Dataset which is maintained regulary and is perhaps one of the largest open imagery dataset at this spatial and temporal resolution. You can find more information about the  Open California project here . These datasets and full-resolution imagery for the entire state of California are covered under a CC BY-SA 4.0 license via Planet's Open California initiative. Your free account allows you to download about 2 GB of dataset each day.", 
            "title": "Registering for a Planet account"
        }, 
        {
            "location": "/projects/rpl/#sign-up-for-a-planet-account", 
            "text": "Planet Explorer  is a powerful tool for exploring Planet's catalog of daily imagery and worldwide mosaics\ndirectly in your browser. It's also your gateway to creating a Planet Account,and gaining access to Planet's APIs.To sign up, visit  planet.com/explorer  and click  Get Started :   Get started with Planet Explorer\nFrom there, click  Sign Up  in the top right of your screen, and enter your email address to receive an invitation:   Sign up with Planet Explorer  Check your email   follow the directions to complete the registration process.", 
            "title": "Sign up for a Planet Account"
        }, 
        {
            "location": "/projects/rpl/#find-your-api-key", 
            "text": "To use Planet's APIs, you'll need an API key. API keys are available to all registered users with active Planet accounts.Once you're signed up, log in to planet.com/account  to get your API key. Find the  API key  field under your account information, as seen here:   Account information (not a real API key)", 
            "title": "Find your API Key"
        }, 
        {
            "location": "/projects/rpl/#registering-for-a-google-earth-engine-account", 
            "text": "If you don\u2019t have a developer account  sign up for one here  and make sure you follow the  instructions  to install the python CLI.   The API and the CLI gets updated frequently and as does the install process as needed so you can read the latest instructions at the page.", 
            "title": "Registering for a Google Earth Engine Account"
        }, 
        {
            "location": "/projects/rpl/#getting-help-with-planet-and-google-earth-engine", 
            "text": "Both Planet and Google Earth Engine maintain a developer page for you to find out more information,test tutorials along with housing a few quick FAQ(s)  You can find  Planet Developer Site here  and offcourse the  Earth Engine Developers Page", 
            "title": "Getting Help with Planet and Google Earth Engine"
        }, 
        {
            "location": "/projects/housekeeping/", 
            "text": "Housekeeping and Setup\n\n\nFor most users data usage often boils down to the software you use to analyze and manipulate images and how you are going to work with them. So here are going to do some housekeeping and setup depending on which tools and setup you are most comfortable with\n\n\n1) Python Setup and libraries\n\n\nDepending on what type of usage you are interested and the libraries you want to use you can find Python installations \nhere\n. And here are some of the libraries you can use with that including but not limited to GDAL, \nScipy\n,\nNumpy\n, \nMatplotlib\n, \nPandas\n, \nFiona\n and \nShapely\n. This list is not exhaustive and include anything else you might need like \nscikit\n or learn \nopencv\n All of \nPypi\n is your oyster\n\n\nfor most systems you can copy and paste this if you have pip on your native command line or terminal\n\n\npip install numpy scipy fiona shapely matplotlib pandas\n\n\nFor Ubuntu and Debian users this might work better and I borrowed it from the installation page at \nScipy\n\n\nsudo apt-get install python-numpy python-scipy python-matplotlib python-pandas\n\n\nNote that GDAL involves a few additional steps for installation on windows machines [You can read more about it here]\n\n\nTwo other tools or setups which might be handy include\n\n\n\n\nvirtualenv\n\n\n\n\nvirtualenv allows the user to create and manage seperate package installations fo multiple projects. Think of this as your new project can have its own set of user libraries seperated from the native python libraries. It allows you to create isolated environments for projects and install packages into that virtual isolated environments.\n\n\nA simple installation would be\n\npip install virtualenv\n\n\nYou can get more details about installation on different operation systems and activation of this environment by \nreading their docs\n.\n\n\n\n\njupyter notebook\n\n\n\n\nThe jupyter hub defined jupyter notebook as\n\n\nThe Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text.\n\n\nA lot of the tool chains and processes have been already built into jupyter notebooks for you to use, and as such installing jupyter notebook maybe useful if you would like to include text,tags, metadata and links within your workflow. You can get \ndetailed installation information here\n\n\n2) Planet and Google Earth Engine(GEE) Command Line Interface(CLI) Setup\n\n\nYou planet account comes with a brand new CLI and it allows you to perfrom basic functions such as search for ID[s] and for images in a specific location, export all image footprint in your area of interest and so on. Installation is pretty simple\n\n\npip install planet\n\n\nYou installation steps from earlier means you have managed to not only create the Google Earth Engine account but also installed its client. Incase you have missed it go to their main reference page for installation of their python client. Since you can consume Earth Engine using both Javascript(in browser) and Python(locally).\n\n\n3) Planet: Open California  \n Disaster Data Collection in GEE\n\n\nFor those who are interested in working with the Open California data within Google Earth Engine, we have ingested and processed imagery for some specific areas within the Open California Dataset and some released as disaster datasets. Each hackathon idea webpage includes lcoation to the AOI and Imagery avaialble for that problem.\n\n\nPlease Note: These are made available to you during the hackathon and maybe migrated in the future.\n\n\n4) Additional Tools and Toolboxes\n\n\nIf you need to handle the data locally using Matlab, QGIS or ArcMap make sure you have these softwares installed. The images can then be downloaded and analyzed using multiple methods and toolsets. A lot of these softwares have additional capabilities to help you further use Planet data. You can find a better reference of external integration here\n\n\n\n\nENVI Integration\n\n\nESRI Integration\n\n\nCesium Integration\n\n\nBoundless\n\n\nPCI Geomatics\n\n\n\n\n5) Adding additional Images\n\n\nFor a minute there imagine you want to work with more data apart from the few areas we talked about, the open california account that you registered for allows you to dowload about 2 GB worth of data anywhere within california and you can then upload it into GEE.\n\n\nFor the simplest users getting images into GEE begins with the Image upload tool located inside GEE. Once you have added the filename you can edit additional metadata such as start time, cloud cover information if you have that from the metadata file among other things. This tool does not have a way for you to ingest any metadata automatically so it has to be fed manually.\n\n\nThe image name is automatically filled in with the filename that you select when uploading.\n\n\n\n\nNote you cannot select more than one image and upload as a single image if they overlap each other. To handle which we have the concept of image collections. Where you can upload many images. To import images into collections, you have to either import them manually as images first and then copy them into the collection one by one or for now use an external tool to help such as using the Google Earth Engine CLI.\n\n\nFor now you can use the tool I made to batch upload collections along with their metadata into Google Earth Engine. You can read about the tool, it's setup and it's operation at \nthis Planet Story\n\n\nIncase you have a Google Cloud Storage bucket you can also push images automatically to be ingested into GEE. Though this requires interaction with gsutil and starting ingestion function for each image. The GEE guide for image ingestion can be \nfound here", 
            "title": "Basic Housekeeping and Setup"
        }, 
        {
            "location": "/projects/housekeeping/#housekeeping-and-setup", 
            "text": "For most users data usage often boils down to the software you use to analyze and manipulate images and how you are going to work with them. So here are going to do some housekeeping and setup depending on which tools and setup you are most comfortable with", 
            "title": "Housekeeping and Setup"
        }, 
        {
            "location": "/projects/housekeeping/#1-python-setup-and-libraries", 
            "text": "Depending on what type of usage you are interested and the libraries you want to use you can find Python installations  here . And here are some of the libraries you can use with that including but not limited to GDAL,  Scipy , Numpy ,  Matplotlib ,  Pandas ,  Fiona  and  Shapely . This list is not exhaustive and include anything else you might need like  scikit  or learn  opencv  All of  Pypi  is your oyster  for most systems you can copy and paste this if you have pip on your native command line or terminal  pip install numpy scipy fiona shapely matplotlib pandas  For Ubuntu and Debian users this might work better and I borrowed it from the installation page at  Scipy  sudo apt-get install python-numpy python-scipy python-matplotlib python-pandas  Note that GDAL involves a few additional steps for installation on windows machines [You can read more about it here]  Two other tools or setups which might be handy include   virtualenv   virtualenv allows the user to create and manage seperate package installations fo multiple projects. Think of this as your new project can have its own set of user libraries seperated from the native python libraries. It allows you to create isolated environments for projects and install packages into that virtual isolated environments.  A simple installation would be pip install virtualenv  You can get more details about installation on different operation systems and activation of this environment by  reading their docs .   jupyter notebook   The jupyter hub defined jupyter notebook as  The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text.  A lot of the tool chains and processes have been already built into jupyter notebooks for you to use, and as such installing jupyter notebook maybe useful if you would like to include text,tags, metadata and links within your workflow. You can get  detailed installation information here", 
            "title": "1) Python Setup and libraries"
        }, 
        {
            "location": "/projects/housekeeping/#2-planet-and-google-earth-enginegee-command-line-interfacecli-setup", 
            "text": "You planet account comes with a brand new CLI and it allows you to perfrom basic functions such as search for ID[s] and for images in a specific location, export all image footprint in your area of interest and so on. Installation is pretty simple  pip install planet  You installation steps from earlier means you have managed to not only create the Google Earth Engine account but also installed its client. Incase you have missed it go to their main reference page for installation of their python client. Since you can consume Earth Engine using both Javascript(in browser) and Python(locally).", 
            "title": "2) Planet and Google Earth Engine(GEE) Command Line Interface(CLI) Setup"
        }, 
        {
            "location": "/projects/housekeeping/#3-planet-open-california-disaster-data-collection-in-gee", 
            "text": "For those who are interested in working with the Open California data within Google Earth Engine, we have ingested and processed imagery for some specific areas within the Open California Dataset and some released as disaster datasets. Each hackathon idea webpage includes lcoation to the AOI and Imagery avaialble for that problem.  Please Note: These are made available to you during the hackathon and maybe migrated in the future.", 
            "title": "3) Planet: Open California  &amp; Disaster Data Collection in GEE"
        }, 
        {
            "location": "/projects/housekeeping/#4-additional-tools-and-toolboxes", 
            "text": "If you need to handle the data locally using Matlab, QGIS or ArcMap make sure you have these softwares installed. The images can then be downloaded and analyzed using multiple methods and toolsets. A lot of these softwares have additional capabilities to help you further use Planet data. You can find a better reference of external integration here   ENVI Integration  ESRI Integration  Cesium Integration  Boundless  PCI Geomatics", 
            "title": "4) Additional Tools and Toolboxes"
        }, 
        {
            "location": "/projects/housekeeping/#5-adding-additional-images", 
            "text": "For a minute there imagine you want to work with more data apart from the few areas we talked about, the open california account that you registered for allows you to dowload about 2 GB worth of data anywhere within california and you can then upload it into GEE.  For the simplest users getting images into GEE begins with the Image upload tool located inside GEE. Once you have added the filename you can edit additional metadata such as start time, cloud cover information if you have that from the metadata file among other things. This tool does not have a way for you to ingest any metadata automatically so it has to be fed manually.  The image name is automatically filled in with the filename that you select when uploading.   Note you cannot select more than one image and upload as a single image if they overlap each other. To handle which we have the concept of image collections. Where you can upload many images. To import images into collections, you have to either import them manually as images first and then copy them into the collection one by one or for now use an external tool to help such as using the Google Earth Engine CLI.  For now you can use the tool I made to batch upload collections along with their metadata into Google Earth Engine. You can read about the tool, it's setup and it's operation at  this Planet Story  Incase you have a Google Cloud Storage bucket you can also push images automatically to be ingested into GEE. Though this requires interaction with gsutil and starting ingestion function for each image. The GEE guide for image ingestion can be  found here", 
            "title": "5) Adding additional Images"
        }, 
        {
            "location": "/projects/downloading-images/", 
            "text": "Downloading Images\n\n\nYou can download images from Planet using a couple of methods, including but not limited to Planet Explorer or using a client to make requests to the Data API and downloading imagery.\n\n\nPlanet Explorer\n\n\nPlanet Explorer is probably one of the most useful and beloved interface to interact with and download Planet Labs satellite imagery. Not only does it allow you to filter your images to specific sensors but it also allows you to filter by cloud cover among other things. A neat little trick in Planet Explorer is that once the images are filtered if you want to download multiple images at once in an order you can hold down the control key(if using a windows machine) and click on multiple sets of imagery adding them to the same order.\n\n\n\n\nSteps to get satellite imagery from Planet Explorer\n\n\nOnce the images have been ordered sit back and relax as the order notification that your delivery is ready to be picked up will be emailed to you.\n\n\nTo interact with the \nData API\n and batch download imagery there is \nhost of Planet Platform documentation\n that teach you how to do that step by step.", 
            "title": "Downloading Images"
        }, 
        {
            "location": "/projects/downloading-images/#downloading-images", 
            "text": "You can download images from Planet using a couple of methods, including but not limited to Planet Explorer or using a client to make requests to the Data API and downloading imagery.", 
            "title": "Downloading Images"
        }, 
        {
            "location": "/projects/downloading-images/#planet-explorer", 
            "text": "Planet Explorer is probably one of the most useful and beloved interface to interact with and download Planet Labs satellite imagery. Not only does it allow you to filter your images to specific sensors but it also allows you to filter by cloud cover among other things. A neat little trick in Planet Explorer is that once the images are filtered if you want to download multiple images at once in an order you can hold down the control key(if using a windows machine) and click on multiple sets of imagery adding them to the same order.   Steps to get satellite imagery from Planet Explorer  Once the images have been ordered sit back and relax as the order notification that your delivery is ready to be picked up will be emailed to you.  To interact with the  Data API  and batch download imagery there is  host of Planet Platform documentation  that teach you how to do that step by step.", 
            "title": "Planet Explorer"
        }, 
        {
            "location": "/projects/getting-images-ee/", 
            "text": "Getting Images into Google Earth Engine\n\n\nFor the simplest users getting images into GEE begins with the Image upload tool located inside GEE. Once you have added the filename you can edit additional metadata such as start time, cloud cover information if you have that from the metadata file among other things. This tool does not have a way for you to ingest any metadata automatically so it has to be fed manually.\n\n\nThe image name is automatically filled in with the filename that you select when uploading.\n\n\n\n\nNote you cannot select more than one image and upload as a single image if they overlap each other. To handle which we have the concept of image collections. Where you can upload many images. To import images into collections, you have to either import them manually as images first and then copy them into the collection one by one or for now use an external tool to help.\n\n\nFor now you can use the tool I made to batch upload collections along with their metadata into Google Earth Engine. You can read about the tool, it's setup and it's operation at \nthis Planet Story\n\n\nIncase you have a Google Cloud Storage bucket you can also push images automatically to be ingested into GEE. Though this requires interaction with gsutil and starting ingestion function for each image. The GEE guide for image ingestion can be \nfound here", 
            "title": "Getting Images into Google Earth Engine"
        }, 
        {
            "location": "/projects/getting-images-ee/#getting-images-into-google-earth-engine", 
            "text": "For the simplest users getting images into GEE begins with the Image upload tool located inside GEE. Once you have added the filename you can edit additional metadata such as start time, cloud cover information if you have that from the metadata file among other things. This tool does not have a way for you to ingest any metadata automatically so it has to be fed manually.  The image name is automatically filled in with the filename that you select when uploading.   Note you cannot select more than one image and upload as a single image if they overlap each other. To handle which we have the concept of image collections. Where you can upload many images. To import images into collections, you have to either import them manually as images first and then copy them into the collection one by one or for now use an external tool to help.  For now you can use the tool I made to batch upload collections along with their metadata into Google Earth Engine. You can read about the tool, it's setup and it's operation at  this Planet Story  Incase you have a Google Cloud Storage bucket you can also push images automatically to be ingested into GEE. Though this requires interaction with gsutil and starting ingestion function for each image. The GEE guide for image ingestion can be  found here", 
            "title": "Getting Images into Google Earth Engine"
        }, 
        {
            "location": "/projects/biomass/", 
            "text": "Change in Biomass: Indicators of Natural Disaster\n\n\nWe are surrounded by natural events and changes in our land use and land cover owing to seasonal variations along with sudden episodic and periodic events. These events for examples natural disasters like cyclones, hurricances and flood events to name a few. Often time the chances of direct observation of these events as they happen are difficult at current temporal frequency, though we keep moving to realizing that. Our next best approach is often looking at the difference before and after an event occurs and more often than not natural vegetation patterns along with the land cover is disturbed or changed after an event. There may also be other proxies such as wetter areas with more area of water, or more sediment that has been deposited over the land and is now visible.\n\n\nData Source\n\n\nWe are providing you with before and after imagery from Puerto Rico before and after the hurricance. These images have been loaded upto onto Google Earth Engine already but if you would like to use them locally they can be downloaded. This is part of our \nPlanet's disaster data initiative\n where we make images available to you for a short windows of time.\n\n\nYou can get the images and the aoi here\n\n\n//Date Range 2017-08-01 to 2017-09-30 (Date of Event 2017-08-28)\n\n\nvar\n \nmaria_aoi\n=\nee\n.\nFeatureCollection\n(\nft:1fnjhy6J3VHOdqbWlXwRjHvEtdrfT2wkwG1Qt5W62\n)\n \n//AOI boundary for San Juan affected by Maria\n\n\nvar\n \nmaria_img\n=\nee\n.\nImageCollection\n(\nprojects/sat-io/Planet/pr_maria_ps\n)\n \n//Image Collection San Juan affected by Maria with PlanetScope 4Band analytic imagery\n\n\n\n\n\n\n\nSuggested Methods\n\n\nOne of the most commonly used indices to develop a direct correlationship with vegetation dynamics is \nNormalized Difference Vegetation Index or NDVI\n. There are additional indicator indices which look for effect of differences in NDVI such as using the \nMisra Soil Index\n. Performing a trend analysis should provides proxies to a sudden event such as Hurricane Irma\n\n\nPerforming a simple NDVI analysis\n\n\nDoing band math in Google Earth Engine is fairly simple and you can call upon simple functions such as sum, divide, subtract\n\n\n//Add the image(note this is specific to your path)\n\n\nvar\n \nimage\n \n=\n \nee\n.\nImage\n(\nprojects/sat-io/Planet/pr_maria_ps/20170803_141049_101f_analytic\n)\n\n\n\n//Perform the NDVI operation\n\n\nvar\n \nndvi\n \n=\n \nimage\n.\nnormalizedDifference\n([\nb4\n,\n \nb3\n]).\nrename\n(\nNDVI\n);\n\n\n\n//Center the Map to your image\n\n\nMap\n.\ncenterObject\n(\nimage\n,\n12\n)\n\n\n\n//Cheat just a bit and add a visualization format\n\n\nvar\n \nvis\n \n=\n \n{\nopacity\n:\n1\n,\nbands\n:\n[\nb4\n,\nb3\n,\nb2\n],\nmin\n:\n304\n,\nmax\n:\n4275\n,\ngamma\n:\n1\n};\n\n\nvar\n \nndviParams\n \n=\n \n{\nmin\n:\n \n-\n1\n,\n \nmax\n:\n \n1\n,\n \npalette\n:\n \n[\ngreen\n,\n \nwhite\n,\n \nblue\n]};\n\n\n\n//Now let\ns add this image and visualize before and after clipping\n\n\nMap\n.\naddLayer\n(\nimage\n,\nvis\n,\nImage\n)\n\n\nMap\n.\naddLayer\n(\nndvi\n,\nndviParams\n,\nImage NDVI\n)\n\n\n\n\n\nWe press run and find a beautiful index for the image\n\n\n\n\nTo run NDVI over the entire collection we create a function and then iterate the function(also called mapping the function) over the entire collection\n\n\nHere is the NDVI function to be applied to the collection\n\nvar\n \naddNDVI\n \n=\n \nfunction\n(\nimage\n)\n \n{\n\n  \nvar\n \nndvi\n \n=\n \nimage\n.\nnormalizedDifference\n([\nb4\n,\n \nb3\n]).\nrename\n(\nNDVI\n);\n\n  \nreturn\n \nndvi\n;\n\n\n};\n\n\n\n\nRunning the function on the overall collection reduces the imagery from a 4 band PlanetScope imagery to a single band NDVI product.\n\n\n\n\nThe overall code now becomes\n\n\nvar\n \nimageCollection\n=\nee\n.\nImageCollection\n(\nusers/planet/planet-sr\n)\n\n\nvar\n \naddNDVI\n \n=\n \nfunction\n(\nimage\n)\n \n{\n\n  \nvar\n \nndvi\n \n=\n \nimage\n.\nnormalizedDifference\n([\nb4\n,\n \nb3\n]).\nrename\n(\nNDVI\n);\n\n  \nreturn\n \nndvi\n;\n\n\n};\n\n\nvar\n \ncollectionNDVI\n=\nimageCollection\n.\nmap\n(\naddNDVI\n)\n\n\n\n//Print the results to understand what is going on\n\n\nprint\n(\nimageCollection\n,\nActual 4 Band collection\n)\n\n\nprint\n(\ncollectionNDVI\n,\nCollection with NDVI performed\n)", 
            "title": "Indicators of Natural Disaster"
        }, 
        {
            "location": "/projects/biomass/#change-in-biomass-indicators-of-natural-disaster", 
            "text": "We are surrounded by natural events and changes in our land use and land cover owing to seasonal variations along with sudden episodic and periodic events. These events for examples natural disasters like cyclones, hurricances and flood events to name a few. Often time the chances of direct observation of these events as they happen are difficult at current temporal frequency, though we keep moving to realizing that. Our next best approach is often looking at the difference before and after an event occurs and more often than not natural vegetation patterns along with the land cover is disturbed or changed after an event. There may also be other proxies such as wetter areas with more area of water, or more sediment that has been deposited over the land and is now visible.", 
            "title": "Change in Biomass: Indicators of Natural Disaster"
        }, 
        {
            "location": "/projects/biomass/#data-source", 
            "text": "We are providing you with before and after imagery from Puerto Rico before and after the hurricance. These images have been loaded upto onto Google Earth Engine already but if you would like to use them locally they can be downloaded. This is part of our  Planet's disaster data initiative  where we make images available to you for a short windows of time.  You can get the images and the aoi here  //Date Range 2017-08-01 to 2017-09-30 (Date of Event 2017-08-28)  var   maria_aoi = ee . FeatureCollection ( ft:1fnjhy6J3VHOdqbWlXwRjHvEtdrfT2wkwG1Qt5W62 )   //AOI boundary for San Juan affected by Maria  var   maria_img = ee . ImageCollection ( projects/sat-io/Planet/pr_maria_ps )   //Image Collection San Juan affected by Maria with PlanetScope 4Band analytic imagery", 
            "title": "Data Source"
        }, 
        {
            "location": "/projects/biomass/#suggested-methods", 
            "text": "One of the most commonly used indices to develop a direct correlationship with vegetation dynamics is  Normalized Difference Vegetation Index or NDVI . There are additional indicator indices which look for effect of differences in NDVI such as using the  Misra Soil Index . Performing a trend analysis should provides proxies to a sudden event such as Hurricane Irma", 
            "title": "Suggested Methods"
        }, 
        {
            "location": "/projects/biomass/#performing-a-simple-ndvi-analysis", 
            "text": "Doing band math in Google Earth Engine is fairly simple and you can call upon simple functions such as sum, divide, subtract  //Add the image(note this is specific to your path)  var   image   =   ee . Image ( projects/sat-io/Planet/pr_maria_ps/20170803_141049_101f_analytic )  //Perform the NDVI operation  var   ndvi   =   image . normalizedDifference ([ b4 ,   b3 ]). rename ( NDVI );  //Center the Map to your image  Map . centerObject ( image , 12 )  //Cheat just a bit and add a visualization format  var   vis   =   { opacity : 1 , bands : [ b4 , b3 , b2 ], min : 304 , max : 4275 , gamma : 1 };  var   ndviParams   =   { min :   - 1 ,   max :   1 ,   palette :   [ green ,   white ,   blue ]};  //Now let s add this image and visualize before and after clipping  Map . addLayer ( image , vis , Image )  Map . addLayer ( ndvi , ndviParams , Image NDVI )   We press run and find a beautiful index for the image   To run NDVI over the entire collection we create a function and then iterate the function(also called mapping the function) over the entire collection  Here is the NDVI function to be applied to the collection var   addNDVI   =   function ( image )   { \n   var   ndvi   =   image . normalizedDifference ([ b4 ,   b3 ]). rename ( NDVI ); \n   return   ndvi ;  };   Running the function on the overall collection reduces the imagery from a 4 band PlanetScope imagery to a single band NDVI product.   The overall code now becomes  var   imageCollection = ee . ImageCollection ( users/planet/planet-sr )  var   addNDVI   =   function ( image )   { \n   var   ndvi   =   image . normalizedDifference ([ b4 ,   b3 ]). rename ( NDVI ); \n   return   ndvi ;  };  var   collectionNDVI = imageCollection . map ( addNDVI )  //Print the results to understand what is going on  print ( imageCollection , Actual 4 Band collection )  print ( collectionNDVI , Collection with NDVI performed )", 
            "title": "Performing a simple NDVI analysis"
        }, 
        {
            "location": "/projects/burn/", 
            "text": "Fire detection and Analysis\n\n\nAs climate change effects the frequency , intentisty and location of forest disturbances, \nforest fires and fires in general\n. These disturbances and forest fires have been studies and records maintained for a long period of time. Infact \nFire and Smoke\n is one of the missions that NASA has and maintains. To be able to study this a Fire Information for Resource Management System (FIRMS)  was creatd which provides a near real time active fire data using Moderate Resolution Imaging Spectroradiometer (MODIS) and the Visible Infrared Imaging Radiometer Suite (VIIRS). This allows the user to ovserve fire events within 3 hours of overpass of the satellite. You can then use higher rsolution imagery to refine and study the same area with higher, spatial, temporal and spectral resolutions. There are more local fires services for fire measurement such as the \nCalifornia Statewide Fire Map\n\n\nData Source\n\n\nThe Open Calfornia datasets contains areas of most common fires within the last year,  La Tuna fire The fire\u2014one of the largest in Los Angeles history\u2014consumed over 7,000 acres of brush from September 1-7, endangering nearby homes, freeways and schools. This is part of our \nOpen California Dataset\n and is available for download. We have also uploaded this imagery to Google Earth Engine for you to be able to pull this automatially.\n\n\nTurns out GEE already processes and ingest FIRMS data for now updated from November 2000- Present and here is their description. You can add the collection by simply adding this line\n\n\n//Date of Incident 2017-09-02\n\n\nvar\n \nlatuna_aoi\n=\nee\n.\nFeatureCollection\n(\nft:1_xiEkvDLlvkrDCnIQ0dfPh60dasj8VTHZs17UT06\n)\n \n//AOI boundary for Latuna Fire\n\n\nvar\n \nlatuna_img\n=\nee\n.\nImageCollection\n(\nprojects/sat-io/Planet/latuna_ps\n)\n \n//Image Collection for La Tuna fire with PlanetScope 4Band analytic imagery\n\n\nvar\n \nfirms\n=\n \nee\n.\nImageCollection\n(\nFIRMS\n)\n \n// Fire Information for Resource Management System\n\n\n\n\n\nIncase you are interested in working with fire at a different location, we have ingested and released some dataset from fire in Argentina. This was released as part of our \nDisaster Monitoring Program\n and you can \nfind more details here.\n\n\n//Date of Incident 2017-01-06\n\n\nvar\n \nargentina_aoi\n=\nee\n.\nFeatureCollection\n(\nft:1y-R9oXLukDlhgyMTLATqmdUZc5uSfJy56W0co7EX\n)\n \n//AOI boundary for Argentina Fire\n\n\nvar\n \nargentina_img\n=\nee\n.\nImageCollection\n(\nprojects/sat-io/Planet/argentina_ps\n)\n \n//Image Collection for Argentina fire with PlanetScope 4Band analytic imagery\n\n\nvar\n \nfirms\n=\n \nee\n.\nImageCollection\n(\nFIRMS\n)\n \n// Fire Information for Resource Management System\n\n\n\n\n\nThe Earth Engine version of the Fire Information for Resource Management System (FIRMS)\ndataset contains the LANCE fire detection product in rasterized form. The near real-time (NRT)\nactive fire locations are processed by LANCE using the standard MODIS MOD14/MYD14 Fire and\nThermal Anomalies product. Each active fire location represents the centroid of a 1km pixel\nthat is flagged by the algorithm as containing one or more fires within the pixel.\nThe data are rasterized as follows: for each FIRMS active fire point,\na 1km bouding box (BB) is defined; pixels in the MODIS sinusoidal projection that intersect\nthe FIRMS BB are identified; if multiple FIRMS BBs intersect the same pixel, the one with higher\nconfidence is retained; in case of a tie, the brighter one is retained.\n\n\n\n\nSuggested Methods\n\n\nFind some of the active firest in our area of interest and look at the effect, for example adding the La Tuna fires over our area of interest can be achieved with the following lines of code. To go to this specific fire events within the FIRMS database try this\n\n\n//Add the collection and filter by Date\n\n\nvar\n \nfirms\n=\n \nee\n.\nImageCollection\n(\nFIRMS\n).\nfilterDate\n(\n2017-09-02\n,\n2017-09-03\n)\n\n\n\n//There is only one image in the collection now so let\ns add the first/only one\n\n\nMap\n.\naddLayer\n(\nee\n.\nImage\n(\nee\n.\nImageCollection\n(\nfirms\n.\nfirst\n())))\n\n\n\n//Set a lat long and zoom level for the setup\n\n\nMap\n.\nsetCenter\n(\n-\n118.3179\n,\n \n34.218\n,\n11\n)\n\n\n\n//To make it look slightly better set the background to be Satellite imagery instead of Map\n\n\nMap\n.\nsetOptions\n(\nSATELLITE\n)\n\n\n\n\n\n\nThink about developing this further, what would change and how woudl you detect the change after an areas has been burnt. Can we use NDVI from my biomass study to study effects on vegetation or are there image differencing methods that give you better information.", 
            "title": "Fire detection and Analysis"
        }, 
        {
            "location": "/projects/burn/#fire-detection-and-analysis", 
            "text": "As climate change effects the frequency , intentisty and location of forest disturbances,  forest fires and fires in general . These disturbances and forest fires have been studies and records maintained for a long period of time. Infact  Fire and Smoke  is one of the missions that NASA has and maintains. To be able to study this a Fire Information for Resource Management System (FIRMS)  was creatd which provides a near real time active fire data using Moderate Resolution Imaging Spectroradiometer (MODIS) and the Visible Infrared Imaging Radiometer Suite (VIIRS). This allows the user to ovserve fire events within 3 hours of overpass of the satellite. You can then use higher rsolution imagery to refine and study the same area with higher, spatial, temporal and spectral resolutions. There are more local fires services for fire measurement such as the  California Statewide Fire Map", 
            "title": "Fire detection and Analysis"
        }, 
        {
            "location": "/projects/burn/#data-source", 
            "text": "The Open Calfornia datasets contains areas of most common fires within the last year,  La Tuna fire The fire\u2014one of the largest in Los Angeles history\u2014consumed over 7,000 acres of brush from September 1-7, endangering nearby homes, freeways and schools. This is part of our  Open California Dataset  and is available for download. We have also uploaded this imagery to Google Earth Engine for you to be able to pull this automatially.  Turns out GEE already processes and ingest FIRMS data for now updated from November 2000- Present and here is their description. You can add the collection by simply adding this line  //Date of Incident 2017-09-02  var   latuna_aoi = ee . FeatureCollection ( ft:1_xiEkvDLlvkrDCnIQ0dfPh60dasj8VTHZs17UT06 )   //AOI boundary for Latuna Fire  var   latuna_img = ee . ImageCollection ( projects/sat-io/Planet/latuna_ps )   //Image Collection for La Tuna fire with PlanetScope 4Band analytic imagery  var   firms =   ee . ImageCollection ( FIRMS )   // Fire Information for Resource Management System   Incase you are interested in working with fire at a different location, we have ingested and released some dataset from fire in Argentina. This was released as part of our  Disaster Monitoring Program  and you can  find more details here.  //Date of Incident 2017-01-06  var   argentina_aoi = ee . FeatureCollection ( ft:1y-R9oXLukDlhgyMTLATqmdUZc5uSfJy56W0co7EX )   //AOI boundary for Argentina Fire  var   argentina_img = ee . ImageCollection ( projects/sat-io/Planet/argentina_ps )   //Image Collection for Argentina fire with PlanetScope 4Band analytic imagery  var   firms =   ee . ImageCollection ( FIRMS )   // Fire Information for Resource Management System   The Earth Engine version of the Fire Information for Resource Management System (FIRMS)\ndataset contains the LANCE fire detection product in rasterized form. The near real-time (NRT)\nactive fire locations are processed by LANCE using the standard MODIS MOD14/MYD14 Fire and\nThermal Anomalies product. Each active fire location represents the centroid of a 1km pixel\nthat is flagged by the algorithm as containing one or more fires within the pixel.\nThe data are rasterized as follows: for each FIRMS active fire point,\na 1km bouding box (BB) is defined; pixels in the MODIS sinusoidal projection that intersect\nthe FIRMS BB are identified; if multiple FIRMS BBs intersect the same pixel, the one with higher\nconfidence is retained; in case of a tie, the brighter one is retained.", 
            "title": "Data Source"
        }, 
        {
            "location": "/projects/burn/#suggested-methods", 
            "text": "Find some of the active firest in our area of interest and look at the effect, for example adding the La Tuna fires over our area of interest can be achieved with the following lines of code. To go to this specific fire events within the FIRMS database try this  //Add the collection and filter by Date  var   firms =   ee . ImageCollection ( FIRMS ). filterDate ( 2017-09-02 , 2017-09-03 )  //There is only one image in the collection now so let s add the first/only one  Map . addLayer ( ee . Image ( ee . ImageCollection ( firms . first ())))  //Set a lat long and zoom level for the setup  Map . setCenter ( - 118.3179 ,   34.218 , 11 )  //To make it look slightly better set the background to be Satellite imagery instead of Map  Map . setOptions ( SATELLITE )    Think about developing this further, what would change and how woudl you detect the change after an areas has been burnt. Can we use NDVI from my biomass study to study effects on vegetation or are there image differencing methods that give you better information.", 
            "title": "Suggested Methods"
        }, 
        {
            "location": "/projects/phrsl/", 
            "text": "Urban Growth and Movement\n\n\nThe Connectivity Lab at Facebook released high resolution population datasets for a couple of countries these are done using high resolution training data at 0.5 m resolution and can be used to understand urban density and movement. This is a 3band product where based on information from \nCenter for International Earth Science Information Network(CIESIN)\n \n\u201cpop,\u201d or Band 1, represents the number of persons estimated to live in each pixel; \u201csettlement\u201d or Band 2, is a mask representing pixels within which one or more buildings was detected with a value of 1; and \u201cclouds,\u201d or Band 3, is a mask representing pixels that contained clouds in the source imagery with a value of 1\n. Apart from this dataset to look at urban dynamics we can use night lights to estimate growth in urban areas and populations such as those from VIIRS dataset. Apart from this there is Gridded Population of the World also made available from CIESIN.\n\n\nData Source\n\n\nWe have ingested 13 High Resolution Settlement Layers(HRSL) layers into Google Earth Engine for you to use during the period of the hackathon. Apart from these you can also look into a special collection bucket that contains specific imagery. You can perform a land use and land cover change analysis to see if urban settlements have increased but apart from that you can also look at Visible Infrared Imaging Radiometer Suite (VIIRS) daily data to look at areas with decreasing or increasing intensity of population chang and dynamics that we measure from these proxies. For this study we are making data for Mexico city available to you for six months period 2017-01-01 to 2017-06-30. This can be compbined with Night Light data and with Open Street Map data to run understand if there are discernable changes within that time period.\n\n\n//Date Range 2016-01-01 to 2017-06-30 at 0-99% cloud cover\n\n\nvar\n \nmexicocity_aoi\n=\nee\n.\nFeatureCollection\n(\nft:1RhOhAGrbUn0vRlrRRFulpfj0MNN6DrGd2PNBC38-\n)\n \n//AOI boundary for Area within Mexico City\n\n\nvar\n \nmexicocity_img\n=\nee\n.\nImageCollection\n(\nprojects/sat-io/Planet/mexicocity_ps\n)\n \n//Image Collection for Mexico City with PlanetScope 4Band analytic imagery\n\n\n\n\n\nSuggested Methods\n\n\nUse a linear trend analysis to determine if certain areas are gaining large clusters of populations and some areas that are slowly losing populations owing to war, genocide, climate migration among other things. One of the simplest application of using the Night time lights dataset to create this can be found at the \nGEE Github page\n and I replicating this here\n\n\nusing the DMSP-OLS Nighttime Lights Time Series Version 4\n\n// Compute the trend of night-time lights.\n\n\n\n// Adds a band containing image date as years since 1991.\n\n\nfunction\n \ncreateTimeBand\n(\nimg\n)\n \n{\n\n  \nvar\n \nyear\n \n=\n \nee\n.\nDate\n(\nimg\n.\nget\n(\nsystem:time_start\n)).\nget\n(\nyear\n).\nsubtract\n(\n1991\n);\n\n  \nreturn\n \nee\n.\nImage\n(\nyear\n).\nbyte\n().\naddBands\n(\nimg\n);\n\n\n}\n\n\n\n// Map the time band creation helper over the night-time lights collection.\n\n\n// https://earthengine.google.org/#detail/NOAA%2FDMSP-OLS%2FNIGHTTIME_LIGHTS\n\n\nvar\n \ncollection\n \n=\n \nee\n.\nImageCollection\n(\nNOAA/DMSP-OLS/NIGHTTIME_LIGHTS\n)\n\n    \n.\nselect\n(\nstable_lights\n)\n\n    \n.\nmap\n(\ncreateTimeBand\n);\n\n\n\n// Visualize brightness in green and a linear fit trend line in red/blue.\n\n\nMap\n.\naddLayer\n(\n\n    \ncollection\n.\nreduce\n(\nee\n.\nReducer\n.\nlinearFit\n()),\n\n    \n{\nmin\n:\n \n0\n,\n \nmax\n:\n \n[\n0.18\n,\n \n20\n,\n \n-\n0.18\n],\n \nbands\n:\n \n[\nscale\n,\n \noffset\n,\n \nscale\n]},\n\n    \nstable lights trend\n);\n\n\n\n\n\n\nI have modified only two lines of code to create the same from VIIRS dataset\n\n\n// Compute the trend of night-time lights.\n\n\n\n// Adds a band containing image date as years since 2014.\n\n\nfunction\n \ncreateTimeBand\n(\nimg\n)\n \n{\n\n  \nvar\n \nyear\n \n=\n \nee\n.\nDate\n(\nimg\n.\nget\n(\nsystem:time_start\n)).\nget\n(\nyear\n).\nsubtract\n(\n2014\n);\n\n  \nreturn\n \nee\n.\nImage\n(\nyear\n).\nbyte\n().\naddBands\n(\nimg\n);\n\n\n}\n\n\n\n// Map the time band creation helper over the night-time lights collection.\n\n\n/var collection = ee.ImageCollection(\nNOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\n)\n\n    \n.\nselect\n(\navg_rad\n)\n\n    \n.\nmap\n(\ncreateTimeBand\n);\n\n\n\n// Visualize brightness in green and a linear fit trend line in red/blue.\n\n\nMap\n.\naddLayer\n(\n\n    \ncollection\n.\nreduce\n(\nee\n.\nReducer\n.\nlinearFit\n()),\n\n    \n{\nmin\n:\n \n0\n,\n \nmax\n:\n \n[\n0.18\n,\n \n20\n,\n \n-\n0.18\n],\n \nbands\n:\n \n[\nscale\n,\n \noffset\n,\n \nscale\n]},\n\n    \nstable lights trend\n);\n\n\n\n\n\n\n\nUse planet data for verification of change in urban area and settlement area in general post assessment of hot spots for analysis.", 
            "title": "Urban Growth and Movement"
        }, 
        {
            "location": "/projects/phrsl/#urban-growth-and-movement", 
            "text": "The Connectivity Lab at Facebook released high resolution population datasets for a couple of countries these are done using high resolution training data at 0.5 m resolution and can be used to understand urban density and movement. This is a 3band product where based on information from  Center for International Earth Science Information Network(CIESIN)   \u201cpop,\u201d or Band 1, represents the number of persons estimated to live in each pixel; \u201csettlement\u201d or Band 2, is a mask representing pixels within which one or more buildings was detected with a value of 1; and \u201cclouds,\u201d or Band 3, is a mask representing pixels that contained clouds in the source imagery with a value of 1 . Apart from this dataset to look at urban dynamics we can use night lights to estimate growth in urban areas and populations such as those from VIIRS dataset. Apart from this there is Gridded Population of the World also made available from CIESIN.", 
            "title": "Urban Growth and Movement"
        }, 
        {
            "location": "/projects/phrsl/#data-source", 
            "text": "We have ingested 13 High Resolution Settlement Layers(HRSL) layers into Google Earth Engine for you to use during the period of the hackathon. Apart from these you can also look into a special collection bucket that contains specific imagery. You can perform a land use and land cover change analysis to see if urban settlements have increased but apart from that you can also look at Visible Infrared Imaging Radiometer Suite (VIIRS) daily data to look at areas with decreasing or increasing intensity of population chang and dynamics that we measure from these proxies. For this study we are making data for Mexico city available to you for six months period 2017-01-01 to 2017-06-30. This can be compbined with Night Light data and with Open Street Map data to run understand if there are discernable changes within that time period.  //Date Range 2016-01-01 to 2017-06-30 at 0-99% cloud cover  var   mexicocity_aoi = ee . FeatureCollection ( ft:1RhOhAGrbUn0vRlrRRFulpfj0MNN6DrGd2PNBC38- )   //AOI boundary for Area within Mexico City  var   mexicocity_img = ee . ImageCollection ( projects/sat-io/Planet/mexicocity_ps )   //Image Collection for Mexico City with PlanetScope 4Band analytic imagery", 
            "title": "Data Source"
        }, 
        {
            "location": "/projects/phrsl/#suggested-methods", 
            "text": "Use a linear trend analysis to determine if certain areas are gaining large clusters of populations and some areas that are slowly losing populations owing to war, genocide, climate migration among other things. One of the simplest application of using the Night time lights dataset to create this can be found at the  GEE Github page  and I replicating this here  using the DMSP-OLS Nighttime Lights Time Series Version 4 // Compute the trend of night-time lights.  // Adds a band containing image date as years since 1991.  function   createTimeBand ( img )   { \n   var   year   =   ee . Date ( img . get ( system:time_start )). get ( year ). subtract ( 1991 ); \n   return   ee . Image ( year ). byte (). addBands ( img );  }  // Map the time band creation helper over the night-time lights collection.  // https://earthengine.google.org/#detail/NOAA%2FDMSP-OLS%2FNIGHTTIME_LIGHTS  var   collection   =   ee . ImageCollection ( NOAA/DMSP-OLS/NIGHTTIME_LIGHTS ) \n     . select ( stable_lights ) \n     . map ( createTimeBand );  // Visualize brightness in green and a linear fit trend line in red/blue.  Map . addLayer ( \n     collection . reduce ( ee . Reducer . linearFit ()), \n     { min :   0 ,   max :   [ 0.18 ,   20 ,   - 0.18 ],   bands :   [ scale ,   offset ,   scale ]}, \n     stable lights trend );    I have modified only two lines of code to create the same from VIIRS dataset  // Compute the trend of night-time lights.  // Adds a band containing image date as years since 2014.  function   createTimeBand ( img )   { \n   var   year   =   ee . Date ( img . get ( system:time_start )). get ( year ). subtract ( 2014 ); \n   return   ee . Image ( year ). byte (). addBands ( img );  }  // Map the time band creation helper over the night-time lights collection.  /var collection = ee.ImageCollection( NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG ) \n     . select ( avg_rad ) \n     . map ( createTimeBand );  // Visualize brightness in green and a linear fit trend line in red/blue.  Map . addLayer ( \n     collection . reduce ( ee . Reducer . linearFit ()), \n     { min :   0 ,   max :   [ 0.18 ,   20 ,   - 0.18 ],   bands :   [ scale ,   offset ,   scale ]}, \n     stable lights trend );    Use planet data for verification of change in urban area and settlement area in general post assessment of hot spots for analysis.", 
            "title": "Suggested Methods"
        }, 
        {
            "location": "/projects/shoreline/", 
            "text": "Shoreline Change Detection\n\n\nShorelines keep changing and evolving and there seems to be global processes that control this building and destruction of coastlines. A lot of impetus has been provided to use the shoreline datasets to measure or estimate retreat and whether these are cyclic changes and/or are these changes persistent and permanent. Earlier studies looking into \nShoreline Changes\n have shown importance of monitoring. As climate change processes change the overall dynamics of short term and long term climate phenomenon, with the increase in relative sea level rise, this concern becomes even more and more critical.\n\n\nData Source\n\n\nThe Open Calfornia datasets contains areas of most common changes for example one along La Jolla Shores. You can add this and start by looking at changes in land and water area over time. This dataset part of our \nOpen California Dataset\n and is available for download. We have also uploaded this imagery to Google Earth Engine for you to be able to pull this automatially.\n\n\n//Date Range 2017-01-01 to 2017-12-31\n\n\nvar\n \nlajolla_aoi\n=\nee\n.\nFeatureCollection\n(\nft:1jPOwve6hzSGJylfvIZPlRH55h8sbDxbs5idcmx5o\n)\n \n//AOI boundary for subset of La Jolla shores\n\n\nvar\n \nlajolla_img\n=\nee\n.\nImageCollection\n(\nprojects/sat-io/Planet/harvey_ps\n)\n \n//Image Collection for subset of La Jolla shores with PlanetScope 4Band analytic imagery\n\n\n\n\n\nTurns out there have been some activity already in terms of using time series shoreline dataset to look at islands. You can read about it in this \nPlanet Story\n about shoreline changes in North Carolina.\n\n\n\n\nSuggested Methods\n\n\nYou can approach this problem in multiple ways including looking at simple image differencing to estimate changes some of which might be seasonal versus episodic changes in the landscape. These images might also be used to look at intertida zones, by combining this imagery along with Landsat and Sentiel-2 optical imagery. You can detect change in overall shoreline area that contribues towards a simple analysis pipeline that could be applied to detect coastline/shoreline loss.", 
            "title": "Shoreline change"
        }, 
        {
            "location": "/projects/shoreline/#shoreline-change-detection", 
            "text": "Shorelines keep changing and evolving and there seems to be global processes that control this building and destruction of coastlines. A lot of impetus has been provided to use the shoreline datasets to measure or estimate retreat and whether these are cyclic changes and/or are these changes persistent and permanent. Earlier studies looking into  Shoreline Changes  have shown importance of monitoring. As climate change processes change the overall dynamics of short term and long term climate phenomenon, with the increase in relative sea level rise, this concern becomes even more and more critical.", 
            "title": "Shoreline Change Detection"
        }, 
        {
            "location": "/projects/shoreline/#data-source", 
            "text": "The Open Calfornia datasets contains areas of most common changes for example one along La Jolla Shores. You can add this and start by looking at changes in land and water area over time. This dataset part of our  Open California Dataset  and is available for download. We have also uploaded this imagery to Google Earth Engine for you to be able to pull this automatially.  //Date Range 2017-01-01 to 2017-12-31  var   lajolla_aoi = ee . FeatureCollection ( ft:1jPOwve6hzSGJylfvIZPlRH55h8sbDxbs5idcmx5o )   //AOI boundary for subset of La Jolla shores  var   lajolla_img = ee . ImageCollection ( projects/sat-io/Planet/harvey_ps )   //Image Collection for subset of La Jolla shores with PlanetScope 4Band analytic imagery   Turns out there have been some activity already in terms of using time series shoreline dataset to look at islands. You can read about it in this  Planet Story  about shoreline changes in North Carolina.", 
            "title": "Data Source"
        }, 
        {
            "location": "/projects/shoreline/#suggested-methods", 
            "text": "You can approach this problem in multiple ways including looking at simple image differencing to estimate changes some of which might be seasonal versus episodic changes in the landscape. These images might also be used to look at intertida zones, by combining this imagery along with Landsat and Sentiel-2 optical imagery. You can detect change in overall shoreline area that contribues towards a simple analysis pipeline that could be applied to detect coastline/shoreline loss.", 
            "title": "Suggested Methods"
        }, 
        {
            "location": "/projects/lulc/", 
            "text": "Land Use and Land Cover Classification\n\n\nUsing multiple sensors with varying resolution sizes to classify the same area often yield difference in the performance of not just the algorithm but also the final result and the overall omission or comission errors that might be present in the imagery. This is further controlled by the behavior of algorithms at scale and the amount of data available for training over a period of time. Though there are existing algorithms for \nsupervised and unsupervise classification in GEE\n results have shown improvement in classification techniques using machine learning and computer visions. A comparison between the performance of multiple classification algorithms points towards the effcts of coarse and fine grid resolution techniques and how we can improve on existing methods.\n\n\nData Source\n\n\nOpen California data can be used for this and has been provided in both GEE and can be downloaded using your newly registered Planet account. Remember that a lot of these training sets are specific for image based on coverage and as a result may not work if applying over extra large collections unless a consistent area is used. You can bring your own classification dataset, by using a GPS to locate specific land cover types that you maybe interested in classifying. We are sharing imagery over the Folsom Lake area along with the dam so you can look at multiple class configuration and classification strategies.\n\n\n//Date Range 2017-01-01 to 2017-12-31\n\n\nvar\n \nfolsom_aoi\n=\nee\n.\nFeatureCollection\n(\nft:1cjxdATGy3NTuR2st8zIJfJFhP8N33ef5j8J6R1qS\n)\n \n//AOI boundary for Folsom Lake\n\n\nvar\n \nfolsom_img\n=\nee\n.\nImageCollection\n(\nprojects/sat-io/Planet/folsom_ps\n)\n \n//Image Collection for Folsom Lake with PlanetScope 4Band analytic imagery\n\n\n\n\n\nSuggested Methods\n\n\nYou can find a lot of literature and methods on optimal ways of classifying an image and each have their own strengths and weaknesses. Look for a couple of things including percentage accuracy if you have ground validation data to verify, time taken to run the algorithm over your imagery, sensor methods and tools you could come up with including applications of an image collection in machine learning algorithms to see any improvements, deep time stacks are useful for training and pre processing. One of the most intersting challenges is often simply cloud and cloud shadow classification, find out ways you maybe able to extract those as classes or masks.\n\n\nHere is a simple Classification and Regression Tree(CART Classification) function to be applied to an image the classes for training for which are selected directly from the image\n\n\n// Merge geometry for feature collections together.\n\n\nvar\n \nnewfc\n \n=\n \ngeometry\n.\nmerge\n(\ngeometry2\n).\nmerge\n(\ngeometry3\n)\n\n  \n// Make the landcover codes be integers starting from 0\n\n  \n.\nremap\n([\n1\n,\n \n2\n,\n3\n],\n \n[\n0\n,\n \n1\n,\n2\n],\n \nlandcover\n);\n\n\nprint\n(\nnewfc\n)\n\n\n// Load Landsat 8 TOA images, get the least cloudy 2015 image.\n\n\nvar\n \nimage\n \n=\n \nee\n.\nImage\n(\nprojects/mdh/Planet/OpenCA/PSScene4Band/20171205_214338_0f06\n)\n\n\nMap\n.\ncenterObject\n(\nimage\n)\n\n\nMap\n.\naddLayer\n(\nimage\n)\n\n\n\n// Use these bands in the prediction.\n\n\nvar\n \nbands\n \n=\n \n[\nB\n,\n \nG\n,\n \nR\n,\n \nN\n];\n\n\n\n// Make training data by \noverlaying\n the points on the image.\n\n\nvar\n \ntraining\n \n=\n \nimage\n.\nselect\n(\nbands\n).\nsampleRegions\n({\n\n  \ncollection\n:\n \nnewfc\n,\n\n  \nproperties\n:\n \n[\nlandcover\n],\n\n  \nscale\n:\n \n3\n\n\n});\n\n\n\n// Get a CART classifier and train it.\n\n\nvar\n \nclassifier\n \n=\n \nee\n.\nClassifier\n.\ncart\n().\ntrain\n({\n\n  \nfeatures\n:\n \ntraining\n,\n\n  \nclassProperty\n:\n \nlandcover\n,\n\n  \ninputProperties\n:\n \nbands\n\n\n});\n\n\n\n// Classify the image.\n\n\nvar\n \nclassified\n \n=\n \nimage\n.\nselect\n(\nbands\n).\nclassify\n(\nclassifier\n);\n\n\n\n// Display the results.\n\n\nMap\n.\ncenterObject\n(\nnewfc\n,\n \n14\n);\n\n\nMap\n.\naddLayer\n(\nimage\n,\n \n{\nbands\n:\n \n[\nN\n,\n \nR\n,\n \nB\n],\n \nmax\n:\n \n2800\n},\n \nPlanetScope image\n);\n\n\nMap\n.\naddLayer\n(\nclassified\n,\n \n{\nmin\n:\n \n0\n,\n \nmax\n:\n \n2\n,\n \npalette\n:\n \n[\n0000FF\n,\n \n00FF00\n,\n \nFF0000\n]},\n \nclassification\n);\n\n\n\n};\n\n\n\n\n\n\n\nResultant image generates a three class classification which can then be checked for accuracy and performance of algorithm given the training data.", 
            "title": "Land use and Land Cover Classification"
        }, 
        {
            "location": "/projects/lulc/#land-use-and-land-cover-classification", 
            "text": "Using multiple sensors with varying resolution sizes to classify the same area often yield difference in the performance of not just the algorithm but also the final result and the overall omission or comission errors that might be present in the imagery. This is further controlled by the behavior of algorithms at scale and the amount of data available for training over a period of time. Though there are existing algorithms for  supervised and unsupervise classification in GEE  results have shown improvement in classification techniques using machine learning and computer visions. A comparison between the performance of multiple classification algorithms points towards the effcts of coarse and fine grid resolution techniques and how we can improve on existing methods.", 
            "title": "Land Use and Land Cover Classification"
        }, 
        {
            "location": "/projects/lulc/#data-source", 
            "text": "Open California data can be used for this and has been provided in both GEE and can be downloaded using your newly registered Planet account. Remember that a lot of these training sets are specific for image based on coverage and as a result may not work if applying over extra large collections unless a consistent area is used. You can bring your own classification dataset, by using a GPS to locate specific land cover types that you maybe interested in classifying. We are sharing imagery over the Folsom Lake area along with the dam so you can look at multiple class configuration and classification strategies.  //Date Range 2017-01-01 to 2017-12-31  var   folsom_aoi = ee . FeatureCollection ( ft:1cjxdATGy3NTuR2st8zIJfJFhP8N33ef5j8J6R1qS )   //AOI boundary for Folsom Lake  var   folsom_img = ee . ImageCollection ( projects/sat-io/Planet/folsom_ps )   //Image Collection for Folsom Lake with PlanetScope 4Band analytic imagery", 
            "title": "Data Source"
        }, 
        {
            "location": "/projects/lulc/#suggested-methods", 
            "text": "You can find a lot of literature and methods on optimal ways of classifying an image and each have their own strengths and weaknesses. Look for a couple of things including percentage accuracy if you have ground validation data to verify, time taken to run the algorithm over your imagery, sensor methods and tools you could come up with including applications of an image collection in machine learning algorithms to see any improvements, deep time stacks are useful for training and pre processing. One of the most intersting challenges is often simply cloud and cloud shadow classification, find out ways you maybe able to extract those as classes or masks.  Here is a simple Classification and Regression Tree(CART Classification) function to be applied to an image the classes for training for which are selected directly from the image  // Merge geometry for feature collections together.  var   newfc   =   geometry . merge ( geometry2 ). merge ( geometry3 ) \n   // Make the landcover codes be integers starting from 0 \n   . remap ([ 1 ,   2 , 3 ],   [ 0 ,   1 , 2 ],   landcover );  print ( newfc )  // Load Landsat 8 TOA images, get the least cloudy 2015 image.  var   image   =   ee . Image ( projects/mdh/Planet/OpenCA/PSScene4Band/20171205_214338_0f06 )  Map . centerObject ( image )  Map . addLayer ( image )  // Use these bands in the prediction.  var   bands   =   [ B ,   G ,   R ,   N ];  // Make training data by  overlaying  the points on the image.  var   training   =   image . select ( bands ). sampleRegions ({ \n   collection :   newfc , \n   properties :   [ landcover ], \n   scale :   3  });  // Get a CART classifier and train it.  var   classifier   =   ee . Classifier . cart (). train ({ \n   features :   training , \n   classProperty :   landcover , \n   inputProperties :   bands  });  // Classify the image.  var   classified   =   image . select ( bands ). classify ( classifier );  // Display the results.  Map . centerObject ( newfc ,   14 );  Map . addLayer ( image ,   { bands :   [ N ,   R ,   B ],   max :   2800 },   PlanetScope image );  Map . addLayer ( classified ,   { min :   0 ,   max :   2 ,   palette :   [ 0000FF ,   00FF00 ,   FF0000 ]},   classification );  };    Resultant image generates a three class classification which can then be checked for accuracy and performance of algorithm given the training data.", 
            "title": "Suggested Methods"
        }, 
        {
            "location": "/projects/clouds/", 
            "text": "Cloud Detection\n\n\nCloud serve as one of the larhest hindrances in optical remote sensing, making it near impossible to look at ground pixels value and depending on opacity often skew the final image values. Not only do clouds by themselves pose a challenge but they are accompanied by issues such as cloud shadow and sometimes haze along with differetiating between cloud and snow in some areas. Our next problem would be a sugegstion to tackle this problem using multiple approaches such as index based as well as machine learning algorithm to look more closely at performance of different algorithms. The idea would be to look for images with clouds near 60-70% of the entire scene and to allow them to think of machine learning algorithms and/or other approaches to detect cloudy versus cloud free area. How do you score per pixel based on what is cloud versus a clear pixel? Last but not least how do you differentiate snow versus cloud in certain areas.\n\n\nData Source\n\n\nWe are providing you with open California data for an area with varying stakcs of cloud cover from 20-80% cloud cover if possible.\n\n\n//Date Range 2016-01-01 to 2018-04-08\n\n\nvar\n \ncloud_aoi\n=\nee\n.\nFeatureCollection\n(\nft:1MENjn5Pakbgf6iXsxppS0W0pUtnbJu6nJLWJoK1B\n)\n \n//AOI boundary for Area with cloud coverage\n\n\nvar\n \ncloud_img\n=\nee\n.\nImageCollection\n(\nprojects/sat-io/Planet/cloud_ps\n)\n \n//Image Collection for Area with cloud coverage with PlanetScope 4Band analytic imagery\n\n\n\n\n\n\n\nSuggested Methods\n\n\nThis is an open ended problem, you can try simple indexing approach and classification methods to machine learning algorithm looking at morphology instead of just the spectral information. The output will probably consist of a binary that could be used as a mask to mask out cloudy areas over the image and works consistenly on multiple images and not just a single image.", 
            "title": "Cloud Detection"
        }, 
        {
            "location": "/projects/clouds/#cloud-detection", 
            "text": "Cloud serve as one of the larhest hindrances in optical remote sensing, making it near impossible to look at ground pixels value and depending on opacity often skew the final image values. Not only do clouds by themselves pose a challenge but they are accompanied by issues such as cloud shadow and sometimes haze along with differetiating between cloud and snow in some areas. Our next problem would be a sugegstion to tackle this problem using multiple approaches such as index based as well as machine learning algorithm to look more closely at performance of different algorithms. The idea would be to look for images with clouds near 60-70% of the entire scene and to allow them to think of machine learning algorithms and/or other approaches to detect cloudy versus cloud free area. How do you score per pixel based on what is cloud versus a clear pixel? Last but not least how do you differentiate snow versus cloud in certain areas.", 
            "title": "Cloud Detection"
        }, 
        {
            "location": "/projects/clouds/#data-source", 
            "text": "We are providing you with open California data for an area with varying stakcs of cloud cover from 20-80% cloud cover if possible.  //Date Range 2016-01-01 to 2018-04-08  var   cloud_aoi = ee . FeatureCollection ( ft:1MENjn5Pakbgf6iXsxppS0W0pUtnbJu6nJLWJoK1B )   //AOI boundary for Area with cloud coverage  var   cloud_img = ee . ImageCollection ( projects/sat-io/Planet/cloud_ps )   //Image Collection for Area with cloud coverage with PlanetScope 4Band analytic imagery", 
            "title": "Data Source"
        }, 
        {
            "location": "/projects/clouds/#suggested-methods", 
            "text": "This is an open ended problem, you can try simple indexing approach and classification methods to machine learning algorithm looking at morphology instead of just the spectral information. The output will probably consist of a binary that could be used as a mask to mask out cloudy areas over the image and works consistenly on multiple images and not just a single image.", 
            "title": "Suggested Methods"
        }, 
        {
            "location": "/projects/flood/", 
            "text": "Flood Impact\n\n\nAs climate change and extreme climate events increase in frequency the notion of safe zones and areas which are rarely affected by natural disaster needs to be redefined. Our current definitions of areas under risk of flood indundation for example is often tied to porximity to flood plains though urban flood freuqency over the past decade points towards loss and impact at non proximal areas as well. The question here would be how do you use Planet's pre and post event dataset to understand standard land cover versus indundation affected areas.\n\n\nData Source\n\n\nWe are providing you with before and after imagery from areas near Houston which was affected by Harvey before and after the hurricance. These images have been loaded upto onto Google Earth Engine(GEE) already but if you would like to use them locally they can be downloaded. This is part of our \nPlanet's disaster data initiative\n where we make images available to you for a short windows of time.\n\n\nYou can get the images and the aoi here\n\n\n//Date Range 2017-08-01 to 2017-09-30 (Date of Event 2017-08-28)\n\n\nvar\n \nharvey_aoi\n=\nee\n.\nFeatureCollection\n(\nft:1WbrmxoQPDsJS35XKwBW7T6kjyjiS7h7d976vNWV7\n)\n \n//AOI boundary for Houston Flooding\n\n\nvar\n \nharvey_img\n=\nee\n.\nImageCollection\n(\nprojects/sat-io/Planet/harvey_ps\n)\n \n//Image Collection for Area with Houston Flooding with PlanetScope 4Band analytic imagery\n\n\n\n\n\n\n\nApart from this the Advanced Rapid Imaging and Analysis (ARIA) team at NASA's Jet Propulsion Laboratory also created flood proxy map to show areas that are indundated using ALOS and Sentinel-1 data which are capable of looking at areas effected using Synthetic Aperture Radar datasets. We have uploaded this to earthengine so that they can be used to mask out areas which were flooded during the event. Population data is also included in GEE using TIGER: US Census Tracts Demographic - Profile 1 Dataset. You can read more about the \ndesignation here\n. You can add these to your code using\n\n\nvar\n \ncensus\n=\nee\n.\nFeatureCollection\n(\nTIGER/2010/Tracts_DP1\n)\n\n\n\n\n\nSuggested Methods\n\n\nNASA(s) developed flood masks\n would be the first step to start but if you find a finer resolution dataset to show flooded areas and extent feel free to use that. You would be able to calculate flooded area in each block group and  you can also integrate census data to estimate number of people affected by this using census data that are available at Block group level within the US. This gives you an estimate of number of people effected and potentially displaced from these areas. A simple analysis for example for an extremely small area results in following metrics which might be a starting point", 
            "title": "Flood Analysis"
        }, 
        {
            "location": "/projects/flood/#flood-impact", 
            "text": "As climate change and extreme climate events increase in frequency the notion of safe zones and areas which are rarely affected by natural disaster needs to be redefined. Our current definitions of areas under risk of flood indundation for example is often tied to porximity to flood plains though urban flood freuqency over the past decade points towards loss and impact at non proximal areas as well. The question here would be how do you use Planet's pre and post event dataset to understand standard land cover versus indundation affected areas.", 
            "title": "Flood Impact"
        }, 
        {
            "location": "/projects/flood/#data-source", 
            "text": "We are providing you with before and after imagery from areas near Houston which was affected by Harvey before and after the hurricance. These images have been loaded upto onto Google Earth Engine(GEE) already but if you would like to use them locally they can be downloaded. This is part of our  Planet's disaster data initiative  where we make images available to you for a short windows of time.  You can get the images and the aoi here  //Date Range 2017-08-01 to 2017-09-30 (Date of Event 2017-08-28)  var   harvey_aoi = ee . FeatureCollection ( ft:1WbrmxoQPDsJS35XKwBW7T6kjyjiS7h7d976vNWV7 )   //AOI boundary for Houston Flooding  var   harvey_img = ee . ImageCollection ( projects/sat-io/Planet/harvey_ps )   //Image Collection for Area with Houston Flooding with PlanetScope 4Band analytic imagery    Apart from this the Advanced Rapid Imaging and Analysis (ARIA) team at NASA's Jet Propulsion Laboratory also created flood proxy map to show areas that are indundated using ALOS and Sentinel-1 data which are capable of looking at areas effected using Synthetic Aperture Radar datasets. We have uploaded this to earthengine so that they can be used to mask out areas which were flooded during the event. Population data is also included in GEE using TIGER: US Census Tracts Demographic - Profile 1 Dataset. You can read more about the  designation here . You can add these to your code using  var   census = ee . FeatureCollection ( TIGER/2010/Tracts_DP1 )", 
            "title": "Data Source"
        }, 
        {
            "location": "/projects/flood/#suggested-methods", 
            "text": "NASA(s) developed flood masks  would be the first step to start but if you find a finer resolution dataset to show flooded areas and extent feel free to use that. You would be able to calculate flooded area in each block group and  you can also integrate census data to estimate number of people affected by this using census data that are available at Block group level within the US. This gives you an estimate of number of people effected and potentially displaced from these areas. A simple analysis for example for an extremely small area results in following metrics which might be a starting point", 
            "title": "Suggested Methods"
        }, 
        {
            "location": "/planet-citations/", 
            "text": "Planet Citations\n\n\nCiting Planet Data\n\n\nFrom a concept in our garage, to operating the largest fleet of Earth-imaging satellites, many people have invested time and energy in developing and enabling access to Planet\u2019s unique data feed. Please cite Planet when using our imagery and tools.\n\n\nTo cite Planet data in publications, please use the following:\n\n\nPlanet Team (2017). Planet Application Program Interface: In Space for Life on Earth. San Francisco, CA. \nhttps://api.planet.com\n.", 
            "title": "Planet Citations"
        }, 
        {
            "location": "/planet-citations/#planet-citations", 
            "text": "", 
            "title": "Planet Citations"
        }, 
        {
            "location": "/planet-citations/#citing-planet-data", 
            "text": "From a concept in our garage, to operating the largest fleet of Earth-imaging satellites, many people have invested time and energy in developing and enabling access to Planet\u2019s unique data feed. Please cite Planet when using our imagery and tools.  To cite Planet data in publications, please use the following:  Planet Team (2017). Planet Application Program Interface: In Space for Life on Earth. San Francisco, CA.  https://api.planet.com .", 
            "title": "Citing Planet Data"
        }, 
        {
            "location": "/featured-projects/", 
            "text": "Featured Projects\n\n\nTell us about your project and we will feature it here allowing you to arrange and display your projects. Learn how to \nPublish your github code on Zenodo and generate a DOI here\n.", 
            "title": "Featured Projects"
        }, 
        {
            "location": "/featured-projects/#featured-projects", 
            "text": "Tell us about your project and we will feature it here allowing you to arrange and display your projects. Learn how to  Publish your github code on Zenodo and generate a DOI here .", 
            "title": "Featured Projects"
        }
    ]
}